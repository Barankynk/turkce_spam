"""
TÃ¼rkÃ§e SMS Spam SÄ±nÄ±flandÄ±rma - Model EÄŸitim Scripti
Bu script veri setini yÃ¼kler, iÅŸler, modeli eÄŸitir ve kaydeder.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import (
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score,
    classification_report,
    confusion_matrix
)
import joblib
import os
import sys

# Kendi modÃ¼llerimizi import et
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from preprocessing import preprocess_message, batch_preprocess
from feature_extraction import TurkishTfidfVectorizer
from utils import plot_confusion_matrix, print_classification_metrics


# Sabitler
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_DIR = os.path.dirname(SCRIPT_DIR)
DATA_PATH = os.path.join(PROJECT_DIR, 'data', 'raw', 'TurkishSMSCollection.csv')
MODEL_PATH = os.path.join(PROJECT_DIR, 'models', 'spam_classifier.joblib')
VECTORIZER_PATH = os.path.join(PROJECT_DIR, 'models', 'tfidf_vectorizer.joblib')
CONFUSION_MATRIX_PATH = os.path.join(PROJECT_DIR, 'data', 'confusion_matrix.png')
TEST_SIZE = 0.2
RANDOM_STATE = 42


def load_dataset(filepath):
    """
    Veri setini yÃ¼kle ve hazÄ±rla.
    
    Args:
        filepath: CSV dosya yolu
        
    Returns:
        DataFrame
    """
    print("\n" + "=" * 70)
    print("VERÄ° SETÄ° YÃœKLEME")
    print("=" * 70)
    
    # CSV'yi yÃ¼kle
    df = pd.read_csv(
        filepath,
        sep=';',
        header=None,
        names=['message', 'group', 'group_text'],
        encoding='utf-8',
        on_bad_lines='skip'
    )
    
    print(f"âœ… Veri seti yÃ¼klendi: {len(df)} mesaj")
    print(f"ğŸ“Š Kolonlar: {list(df.columns)}")
    
    # Group kolonunu gÃ¼venli ÅŸekilde integer'a Ã§evir
    df['group'] = pd.to_numeric(df['group'], errors='coerce')
    
    # GeÃ§ersiz satÄ±rlarÄ± at
    initial_count = len(df)
    df = df.dropna(subset=['group'])
    removed_invalid = initial_count - len(df)
    if removed_invalid > 0:
        print(f"âš ï¸ {removed_invalid} adet geÃ§ersiz group deÄŸeri temizlendi")
    
    df['group'] = df['group'].astype(int)
    
    # Group'u binary hale getir (1 = Spam, 0 = Normal)
    # Group 1 = Spam, Group 2 = Normal
    # Explicit mapping ile label oluÅŸtur
    df['label'] = df['group'].map({1: 1, 2: 0})
    
    # Beklenmeyen deÄŸerleri temizle (1 veya 2 dÄ±ÅŸÄ±nda)
    initial_count = len(df)
    df = df.dropna(subset=['label'])
    removed_unexpected = initial_count - len(df)
    if removed_unexpected > 0:
        print(f"âš ï¸ {removed_unexpected} adet beklenmeyen group deÄŸeri (1 veya 2 dÄ±ÅŸÄ±nda) temizlendi")
    
    df['label'] = df['label'].astype(int)
    
    # SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±
    spam_count = (df['label'] == 1).sum()
    normal_count = (df['label'] == 0).sum()
    
    print(f"\nğŸ“ˆ SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:")
    print(f"  Spam: {spam_count} (%{spam_count/len(df)*100:.1f})")
    print(f"  Normal: {normal_count} (%{normal_count/len(df)*100:.1f})")
    
    return df


def preprocess_data(df):
    """
    Metinleri Ã¶n iÅŸlemden geÃ§ir.
    
    Args:
        df: DataFrame
        
    Returns:
        DataFrame with processed messages
    """
    print("\n" + "=" * 70)
    print("METIN Ã–N Ä°ÅLEME")
    print("=" * 70)
    
    print("ğŸ”„ Metinler iÅŸleniyor...")
    
    # Batch preprocessing
    df['processed_message'] = batch_preprocess(
        df['message'].tolist(),
        remove_punct=True,
        normalize_nums=True,  # SayÄ±larÄ± <NUM> token'a Ã§evir
        remove_stop=True,
        use_stemming=True,
        advanced_stem=True  # Advanced stemming kullan (varsayÄ±lan)
    )
    
    # BoÅŸ mesajlarÄ± filtrele
    initial_count = len(df)
    df = df[df['processed_message'].str.len() > 0]
    removed = initial_count - len(df)
    
    if removed > 0:
        print(f"âš ï¸ {removed} adet boÅŸ mesaj kaldÄ±rÄ±ldÄ±")
    
    print(f"âœ… {len(df)} mesaj baÅŸarÄ±yla iÅŸlendi")
    
    # Ã–rnek gÃ¶ster
    print("\nğŸ“ Ã–rnek Ä°ÅŸlenmiÅŸ Mesajlar:")
    for i in range(min(3, len(df))):
        print(f"\n{i+1}. HAM: {df.iloc[i]['message'][:80]}...")
        print(f"   Ä°ÅLENMÄ°Å: {df.iloc[i]['processed_message'][:80]}...")
        print(f"   ETÄ°KET: {'SPAM' if df.iloc[i]['label'] == 1 else 'NORMAL'}")
    
    return df


def split_dataset(df, test_size=0.2, random_state=42):
    """
    Veri setini train/test olarak ayÄ±r.
    
    Args:
        df: DataFrame
        test_size: Test seti oranÄ±
        random_state: Random seed
        
    Returns:
        X_train, X_test, y_train, y_test
    """
    print("\n" + "=" * 70)
    print("VERÄ° SETÄ° AYIRMA")
    print("=" * 70)
    
    X = df['processed_message']
    y = df['label']
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=test_size,
        random_state=random_state,
        stratify=y  # SÄ±nÄ±f dengesini koru
    )
    
    print(f"âœ… EÄŸitim seti: {len(X_train)} mesaj")
    print(f"âœ… Test seti: {len(X_test)} mesaj")
    print(f"ğŸ“Š Train Spam oranÄ±: %{(y_train.sum()/len(y_train)*100):.1f}")
    print(f"ğŸ“Š Test Spam oranÄ±: %{(y_test.sum()/len(y_test)*100):.1f}")
    
    return X_train, X_test, y_train, y_test


def train_model(X_train, y_train, X_test, y_test):
    """
    TF-IDF vektÃ¶rleÅŸtir ve Multinomial Naive Bayes eÄŸit.
    
    Args:
        X_train: EÄŸitim metinleri
        y_train: EÄŸitim etiketleri
        X_test: Test metinleri
        y_test: Test etiketleri
        
    Returns:
        model, vectorizer, metrics
    """
    print("\n" + "=" * 70)
    print("TF-IDF VEKTÃ–RLEÅTÄ°RME")
    print("=" * 70)
    
    # TF-IDF Vectorizer oluÅŸtur
    vectorizer = TurkishTfidfVectorizer(
        max_features=5000,
        ngram_range=(1, 2),  # Unigram ve bigram
        min_df=2,
        max_df=0.95
    )
    
    # Fit ve transform
    X_train_tfidf = vectorizer.fit_transform(X_train.tolist())
    X_test_tfidf = vectorizer.transform(X_test.tolist())
    
    print(f"âœ… TF-IDF matris boyutu: {X_train_tfidf.shape}")
    print(f"ğŸ“š Vocabulary boyutu: {vectorizer.get_vocabulary_size()}")
    
    print("\n" + "=" * 70)
    print("MODEL EÄÄ°TÄ°MÄ°")
    print("=" * 70)
    
    # Multinomial Naive Bayes
    model = MultinomialNB(alpha=1.0)
    
    print("ğŸ”„ Model eÄŸitiliyor...")
    model.fit(X_train_tfidf, y_train)
    print("âœ… Model eÄŸitimi tamamlandÄ±!")
    
    print("\n" + "=" * 70)
    print("MODEL DEÄERLENDÄ°RME")
    print("=" * 70)
    
    # Tahmin
    y_train_pred = model.predict(X_train_tfidf)
    y_test_pred = model.predict(X_test_tfidf)
    
    # Metrikler
    train_accuracy = accuracy_score(y_train, y_train_pred)
    test_accuracy = accuracy_score(y_test, y_test_pred)
    
    test_precision = precision_score(y_test, y_test_pred)
    test_recall = recall_score(y_test, y_test_pred)
    test_f1 = f1_score(y_test, y_test_pred)
    
    print(f"\nğŸ“Š EÄITIM SETÄ°:")
    print(f"  Accuracy: %{train_accuracy*100:.2f}")
    
    print(f"\nğŸ“Š TEST SETÄ°:")
    print(f"  Accuracy:  %{test_accuracy*100:.2f}")
    print(f"  Precision: %{test_precision*100:.2f}")
    print(f"  Recall:    %{test_recall*100:.2f}")
    print(f"  F1-Score:  %{test_f1*100:.2f}")
    
    # DetaylÄ± classification report
    print_classification_metrics(y_test, y_test_pred, target_names=['Normal', 'Spam'])
    
    # Confusion Matrix
    print("\nğŸ“Š CONFUSION MATRIX:")
    cm = confusion_matrix(y_test, y_test_pred)
    print(cm)
    
    # GÃ¶rselleÅŸtirme
    plot_confusion_matrix(
        y_test, 
        y_test_pred,
        labels=['Normal', 'Spam'],
        save_path=CONFUSION_MATRIX_PATH
    )
    
    metrics = {
        'train_accuracy': train_accuracy,
        'test_accuracy': test_accuracy,
        'precision': test_precision,
        'recall': test_recall,
        'f1_score': test_f1
    }
    
    return model, vectorizer, metrics


def save_models(model, vectorizer, model_path, vectorizer_path):
    """
    Model ve vectorizer'Ä± kaydet.
    
    Args:
        model: EÄŸitilmiÅŸ model
        vectorizer: TF-IDF vectorizer
        model_path: Model kayÄ±t yolu
        vectorizer_path: Vectorizer kayÄ±t yolu
    """
    print("\n" + "=" * 70)
    print("MODEL KAYDETME")
    print("=" * 70)
    
    # KlasÃ¶rÃ¼ oluÅŸtur
    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    
    # Modeli kaydet
    joblib.dump(model, model_path)
    print(f"âœ… Model kaydedildi: {model_path}")
    
    # Vectorizer'Ä± kaydet
    vectorizer.save(vectorizer_path)
    
    print("\nâœ… TÃ¼m dosyalar baÅŸarÄ±yla kaydedildi!")


def main():
    """Ana eÄŸitim pipeline'Ä±."""
    print("\n" + "=" * 70)
    print("TÃœRKÃ‡E SMS SPAM TESPÄ°TÄ° - MODEL EÄÄ°TÄ°MÄ°")
    print("=" * 70)
    
    # 1. Veri yÃ¼kle
    df = load_dataset(DATA_PATH)
    
    # 2. Ã–n iÅŸleme
    df = preprocess_data(df)
    
    # 3. Train/Test ayÄ±r
    X_train, X_test, y_train, y_test = split_dataset(
        df, 
        test_size=TEST_SIZE, 
        random_state=RANDOM_STATE
    )
    
    # 4. Model eÄŸit
    model, vectorizer, metrics = train_model(X_train, y_train, X_test, y_test)
    
    # 5. Modelleri kaydet
    save_models(model, vectorizer, MODEL_PATH, VECTORIZER_PATH)
    
    # Ã–zet
    print("\n" + "=" * 70)
    print("Ã–ZET RAPOR")
    print("=" * 70)
    print(f"âœ… Toplam mesaj: {len(df)}")
    print(f"âœ… Test Accuracy: %{metrics['test_accuracy']*100:.2f}")
    print(f"âœ… F1-Score: %{metrics['f1_score']*100:.2f}")
    print(f"âœ… Model: {MODEL_PATH}")
    print(f"âœ… Vectorizer: {VECTORIZER_PATH}")
    print("=" * 70)
    
    print("\nğŸ‰ MODEL EÄÄ°TÄ°MÄ° BAÅARIYLA TAMAMLANDI!")


if __name__ == "__main__":
    main()
